{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46fd47ad-0a97-4d5a-a6c7-3a358a23b42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f21cd17a-60f4-49da-b36f-8da47e17b769",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf12f47a-c69e-4675-b3f6-5592e5b839a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c50340-5611-4aa7-b562-96fd63850bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.random.normal(0, 1, size=X_new_ori.shape)\n",
    "X_new = X_new_ori + z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d094862d-745e-4724-b91a-941ce4e1eb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "587fe984-54cc-46b1-aa83-30837dd96938",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size = 0.2, random_state = 42)\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfae44c8-539b-484f-aadb-61ba1e8bb50d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "40629fda-03ae-4a16-b5f9-6de7ee7d3f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bp</th>\n",
       "      <th>Sg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Su</th>\n",
       "      <th>Rbc</th>\n",
       "      <th>Bu</th>\n",
       "      <th>Sc</th>\n",
       "      <th>Sod</th>\n",
       "      <th>Pot</th>\n",
       "      <th>Hemo</th>\n",
       "      <th>Wbcc</th>\n",
       "      <th>Rbcc</th>\n",
       "      <th>Htn</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.00000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.00000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>75.956250</td>\n",
       "      <td>1.017750</td>\n",
       "      <td>0.99375</td>\n",
       "      <td>0.384375</td>\n",
       "      <td>0.89375</td>\n",
       "      <td>55.386563</td>\n",
       "      <td>2.873625</td>\n",
       "      <td>137.464750</td>\n",
       "      <td>4.680781</td>\n",
       "      <td>12.589656</td>\n",
       "      <td>8422.937500</td>\n",
       "      <td>4.719031</td>\n",
       "      <td>0.352313</td>\n",
       "      <td>0.618750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.833242</td>\n",
       "      <td>0.005494</td>\n",
       "      <td>1.25694</td>\n",
       "      <td>1.019745</td>\n",
       "      <td>0.30864</td>\n",
       "      <td>48.875895</td>\n",
       "      <td>5.424476</td>\n",
       "      <td>9.778089</td>\n",
       "      <td>3.125634</td>\n",
       "      <td>2.662750</td>\n",
       "      <td>2563.656577</td>\n",
       "      <td>0.815552</td>\n",
       "      <td>0.476909</td>\n",
       "      <td>0.486454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.005000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>70.000000</td>\n",
       "      <td>1.015000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.900000</td>\n",
       "      <td>6900.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>76.000000</td>\n",
       "      <td>1.020000</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>137.530000</td>\n",
       "      <td>4.630000</td>\n",
       "      <td>12.530000</td>\n",
       "      <td>8406.000000</td>\n",
       "      <td>4.710000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>80.000000</td>\n",
       "      <td>1.020000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>2.925000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>14.700000</td>\n",
       "      <td>9400.000000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>180.000000</td>\n",
       "      <td>1.025000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>391.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>17.800000</td>\n",
       "      <td>26400.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Bp          Sg         Al          Su        Rbc          Bu  \\\n",
       "count  320.000000  320.000000  320.00000  320.000000  320.00000  320.000000   \n",
       "mean    75.956250    1.017750    0.99375    0.384375    0.89375   55.386563   \n",
       "std     13.833242    0.005494    1.25694    1.019745    0.30864   48.875895   \n",
       "min     50.000000    1.005000    0.00000    0.000000    0.00000   10.000000   \n",
       "25%     70.000000    1.015000    0.00000    0.000000    1.00000   27.000000   \n",
       "50%     76.000000    1.020000    0.50000    0.000000    1.00000   42.000000   \n",
       "75%     80.000000    1.020000    2.00000    0.000000    1.00000   57.000000   \n",
       "max    180.000000    1.025000    5.00000    5.000000    1.00000  391.000000   \n",
       "\n",
       "               Sc         Sod         Pot        Hemo          Wbcc  \\\n",
       "count  320.000000  320.000000  320.000000  320.000000    320.000000   \n",
       "mean     2.873625  137.464750    4.680781   12.589656   8422.937500   \n",
       "std      5.424476    9.778089    3.125634    2.662750   2563.656577   \n",
       "min      0.400000    4.500000    2.500000    3.100000   2200.000000   \n",
       "25%      0.900000  135.000000    4.000000   10.900000   6900.000000   \n",
       "50%      1.350000  137.530000    4.630000   12.530000   8406.000000   \n",
       "75%      2.925000  140.250000    4.800000   14.700000   9400.000000   \n",
       "max     76.000000  163.000000   47.000000   17.800000  26400.000000   \n",
       "\n",
       "             Rbcc         Htn       Class  \n",
       "count  320.000000  320.000000  320.000000  \n",
       "mean     4.719031    0.352313    0.618750  \n",
       "std      0.815552    0.476909    0.486454  \n",
       "min      2.100000    0.000000    0.000000  \n",
       "25%      4.500000    0.000000    0.000000  \n",
       "50%      4.710000    0.000000    1.000000  \n",
       "75%      5.100000    1.000000    1.000000  \n",
       "max      8.000000    1.000000    1.000000  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30814598-8ba6-4f19-9878-347bad4cacec",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['Class']\n",
    "X_train = train.loc[:, train.columns!='Class']\n",
    "y_test = test['Class']\n",
    "X_test = test.loc[:, test.columns!='Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "62faf9c5-f088-4174-9ca6-984558dfd512",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new2 = pd.DataFrame(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "64e0de1f-58bf-4388-8974-40f25ac8d18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgt = LogisticRegression(random_state=42, penalty='none')\n",
    "lgt.fit(X_new2, y_train)\n",
    "np.mean(lgt.predict(X_test_new) == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60a9928a-7982-45d8-a2ef-340810c32754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(algo, X, Y, M, K, criteria):\n",
    "    \"\"\"\n",
    "    This function uses dropout method to train a predicted model that optimizes the specific criterion.\n",
    "    @param algo(required): A learning algorithm that takes X, Y as inputs and outputs a model \n",
    "    @param X(required): The matrix of indepdent/predictor variables\n",
    "    @param Y(required): The outcome variable\n",
    "    @param M(required): The number of Monte Carlo replicates\n",
    "    @param K(required): Number of CV folds for tuning hyperparameter\n",
    "    @param criteria(required): A criterion to be used to evaluate the method. \n",
    "    Can be either mean squared error (MSE) or mean absolute deviation (MAD)\n",
    "    @return: A predictive model that optimizes the specific criterion\n",
    "    \"\"\"\n",
    "    \n",
    "    # List of phi values to be tuned\n",
    "    phi_list = np.arange(0, 1, 0.1)\n",
    "    cv_error = []\n",
    "    \n",
    "    for phi in phi_list:\n",
    "        # Create k folds for cross-validation\n",
    "        kf = KFold(n_splits = K, shuffle=True)\n",
    "        error = []\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "            \n",
    "            # Monte Carlo: expand original data M times to generate a new dataset \n",
    "            # (X will be modified later)\n",
    "            X_new_ori = np.repeat(X_train, M, axis=0)\n",
    "            Y_new = np.repeat(Y_train, M, axis=0)\n",
    "            \n",
    "            # Generate independent Bernoulli random variables Z\n",
    "            z = np.random.binomial(1, phi, size=X_new_ori.shape)\n",
    "            # Random Dropout for X\n",
    "            X_new = X_new_ori*z/(1-phi)\n",
    "            \n",
    "            # Fit input learning algorithm with new data\n",
    "            reg = algo(X_new, Y_new)\n",
    "            # Predict Y using X test\n",
    "            pred = reg.predict(X_test)\n",
    "            \n",
    "            # Calculate MSE/MAD using the model obtained above\n",
    "            if criteria == \"MSE\":\n",
    "                error.append(mean_squared_error(Y_test, pred))\n",
    "            elif criteria == \"MAD\":\n",
    "                error.append(mean_absolute_error(Y_test, pred))\n",
    "            # Raise error if input is not MSE or MAD\n",
    "            else:\n",
    "                raise ValueError('Please input either MSE or MAD!')\n",
    "        \n",
    "        cv_error.append(np.mean(error))\n",
    "    \n",
    "    # Find optimal phi value that has the smallest CV error\n",
    "    phi_opt = phi_list[np.argmin(cv_error)]\n",
    "    \n",
    "    # Use the optimal phi value to train the predicted model\n",
    "    X_new_ori = np.repeat(X, M, axis=0)\n",
    "    Y_new = np.repeat(Y, M, axis=0)\n",
    "    \n",
    "    z = np.random.binomial(1, phi_opt, size=X_new_ori.shape)\n",
    "    X_new = X_new_ori*z/(1-phi_opt)\n",
    "\n",
    "    reg = algo(X_new, Y_new)\n",
    "    \n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30f0cde2-d657-4bb0-8964-b9c9c168df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgt_drop(X, Y):\n",
    "    return LogisticRegression(random_state=42, penalty='none').fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c00ad58-70af-4732-afe5-120cf693a473",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train.values\n",
    "y = y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cc7b9f7-683c-44f5-b7cb-87e6a9d0982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.random.normal(0, 3, size=X.shape)\n",
    "X_new = X + z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d111dada-8392-4934-9f3f-375c333826db",
   "metadata": {},
   "outputs": [],
   "source": [
    "z2 = np.random.normal(0, 20, size=X_test.shape)\n",
    "X_test_new = X_test + z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f798b52d-a1e8-4b1f-996a-c81829422df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgt_drop_fit = dropout(lgt_drop, X_new, y, 100, 5, 'MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e64251d0-fdc5-4a4b-8479-8ddec1afa48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6625"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lgt_drop_fit.predict(X_test_new) == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265cfb46-6d27-4549-95ee-0d1eb4ed6a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_updated(algo, X, Y, M, K, criteria):\n",
    "    \"\"\"\n",
    "    This function uses dropout method to train a predicted model that optimizes the specific criterion.\n",
    "    @param algo(required): A learning algorithm that takes X, Y as inputs and outputs a model \n",
    "    @param X(required): The matrix of indepdent/predictor variables\n",
    "    @param Y(required): The outcome variable\n",
    "    @param M(required): The number of Monte Carlo replicates\n",
    "    @param K(required): Number of CV folds for tuning hyperparameter\n",
    "    @param criteria(required): A criterion to be used to evaluate the method. \n",
    "    Can be either mean squared error (MSE) or mean absolute deviation (MAD)\n",
    "    @return: A predictive model that optimizes the specific criterion\n",
    "    \"\"\"\n",
    "    \n",
    "    # List of phi values to be tuned\n",
    "    cv_error = []\n",
    "    # Create k folds for cross-validation\n",
    "    kf = KFold(n_splits = K, shuffle=True)\n",
    "    error = []\n",
    "    p_up_list = np.arange(0.5, 1, 0.1)\n",
    "    \n",
    "    for p_up in p_up_list:\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "            # Monte Carlo: expand original data M times to generate a new dataset \n",
    "            # (X will be modified later)\n",
    "            X_new_ori = np.repeat(X_train, M, axis=0)\n",
    "            Y_new = np.repeat(Y_train, M, axis=0)\n",
    "\n",
    "            # Generate independent Bernoulli random variables Z\n",
    "            center = np.mean(X_new_ori, axis=0)\n",
    "            dist = [distance.euclidean(point, center) for point in X_new_ori]\n",
    "            quant_list = np.quantile(dist, [0.25, 0.5, 0.75])\n",
    "            dist_df = pd.DataFrame({'dist': dist})\n",
    "            dist_df['category'] = pd.cut(dist_df['dist'], quant_list, \n",
    "                                         labels=['0-25%', '25-50%', '50-75%', '75-100%'])\n",
    "            dist_df['prob'] = 0\n",
    "            dist_df.loc[dist_df.category == '0-25%', 'prob'] = p_up\n",
    "            dist_df.loc[dist_df.category == '25-50%', 'prob'] = p_up - 0.1\n",
    "            dist_df.loc[dist_df.category == '50-75%', 'prob'] = p_up - 0.2\n",
    "            dist_df.loc[dist_df.category == '75-100%', 'prob'] = p_up - 0.3\n",
    "            phi_list = dist_df['prob']\n",
    "            z_list = np.random.binomial(1, phi_list, size=X_new_ori.shape[0])\n",
    "\n",
    "            # Random Dropout for X\n",
    "            X_new = X_new_ori*z_list/(1-phi_list)\n",
    "\n",
    "            # Fit input learning algorithm with new data\n",
    "            reg = algo(X_new, Y_new)\n",
    "            # Predict Y using X test\n",
    "            pred = reg.predict(X_test)\n",
    "\n",
    "            # Calculate MSE/MAD using the model obtained above\n",
    "            if criteria == \"MSE\":\n",
    "                error.append(mean_squared_error(Y_test, pred))\n",
    "            elif criteria == \"MAD\":\n",
    "                error.append(mean_absolute_error(Y_test, pred))\n",
    "            # Raise error if input is not MSE or MAD\n",
    "            else:\n",
    "                raise ValueError('Please input either MSE or MAD!')\n",
    "\n",
    "        cv_error.append(np.mean(error))\n",
    "    \n",
    "    # Find optimal p_up value that has the smallest CV error\n",
    "    p_up_opt = p_up_list[np.argmin(cv_error)]\n",
    "    \n",
    "    # Use the optimal phi value to train the predicted model\n",
    "    X_new_ori = np.repeat(X, M, axis=0)\n",
    "    Y_new = np.repeat(Y, M, axis=0)\n",
    "    \n",
    "    # Generate independent Bernoulli random variables Z\n",
    "    center = np.mean(X_new_ori, axis=0)\n",
    "    dist = [distance.euclidean(point, center) for point in X_new_ori]\n",
    "    quant_list = np.quantile(dist, [0.25, 0.5, 0.75])\n",
    "    dist_df = pd.DataFrame({'dist': dist})\n",
    "    dist_df['category'] = pd.cut(dist_df['dist'], quant_list, \n",
    "                                 labels=['0-25%', '25-50%', '50-75%', '75-100%'])\n",
    "    dist_df['prob'] = 0\n",
    "    dist_df.loc[dist_df.category == '0-25%', 'prob'] = p_up_opt\n",
    "    dist_df.loc[dist_df.category == '25-50%', 'prob'] = p_up_opt - 0.1\n",
    "    dist_df.loc[dist_df.category == '50-75%', 'prob'] = p_up_opt - 0.2\n",
    "    dist_df.loc[dist_df.category == '75-100%', 'prob'] = p_up_opt - 0.3\n",
    "    phi_list = dist_df['prob']\n",
    "    z_list = np.random.binomial(1, phi_list, size=X_new_ori.shape[0])\n",
    "\n",
    "    # Random Dropout for X\n",
    "    X_new = X_new_ori*z_list/(1-phi_list)\n",
    "\n",
    "    # Fit input learning algorithm with new data\n",
    "    reg = algo(X_new, Y_new)\n",
    "    \n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f30602d3-6aa3-4d27-aeea-1110d99d82f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "center = np.mean(X_new, axis=0)\n",
    "dist = [distance.euclidean(point, center) for point in X_new]\n",
    "quant_list = np.quantile(dist, [0, 0.25, 0.5, 0.75, 1])\n",
    "cat_four = pd.cut(dist, quant_list, \n",
    "                              labels=['0-25%', '25-50%', '50-75%', '75-100%'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0c14b4a-2117-4daf-b9e2-ab53e1a0243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_df = pd.DataFrame({'dist': dist})\n",
    "dist_df['category'] = pd.cut(dist_df['dist'], quant_list, labels=['0-25%', '25-50%', '50-75%', '75-100%'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "51c076e1-356f-4c0c-8ab0-0d6fb748f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_df['prob'] = 0\n",
    "dist_df.loc[dist_df.category == '0-25%', 'prob'] = 0.1\n",
    "dist_df.loc[dist_df.category == '25-50%', 'prob'] = 0.2\n",
    "dist_df.loc[dist_df.category == '50-75%', 'prob'] = 0.3\n",
    "dist_df.loc[dist_df.category == '75-100%', 'prob'] = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "713a25a5-994e-44d7-bbf1-4e0a67445749",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_list = dist_df['prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0c87ad3b-073e-441c-88e3-ae2b99bb72e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f0ad6c68-d585-451f-adfb-3d669e6b38d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_list = np.random.binomial(1, phi_list, size=X_new.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "603dab4b-daa4-4103-8a62-73f3fc96ff12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b7a1101-de71-4050-92df-c0debb157953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['50-75%', '75-100%', '0-25%', '50-75%', '0-25%', ..., '75-100%', '50-75%', '50-75%', '25-50%', '50-75%']\n",
       "Length: 320\n",
       "Categories (4, object): ['0-25%' < '25-50%' < '50-75%' < '75-100%']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62cbf7ab-8bc8-47dc-9430-e8afd168ddb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.78149680e+01, 5.55483572e+01, 1.17874644e+03, 2.30720834e+03,\n",
       "       1.79747725e+04])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1dfd920f-141a-4d15-bf7b-1cf1b957d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_four = pd.cut(dist, quant_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "125e93f0-2291-4c96-99bd-e558db48ae63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Interval(1178.746, 2307.208, closed='right')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_four[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86553360-f18a-43cf-80f0-c671af375ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {'x': X_new, 'category': cat_four}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd2f80ad-9b8c-47ff-9daf-3b657e22624e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': array([[ 7.13730500e+01, -2.10885796e+00,  8.01682114e+00, ...,\n",
       "          6.70118972e+03,  7.39424088e+00,  1.79700687e+00],\n",
       "        [ 1.03694526e+02,  5.97891267e-01,  2.44743996e-01, ...,\n",
       "          1.14040797e+04, -2.91921745e+00,  1.00484074e+00],\n",
       "        [ 5.87236062e+01,  2.43315179e+00,  8.21343060e+00, ...,\n",
       "          8.40387538e+03,  1.35804866e+00,  1.92310447e+00],\n",
       "        ...,\n",
       "        [ 8.09615736e+01, -2.06199581e+00,  9.09108980e-01, ...,\n",
       "          7.19887525e+03,  6.62937974e+00,  2.57408873e+00],\n",
       "        [ 7.87718297e+01, -4.24170747e+00, -3.29828939e-01, ...,\n",
       "          7.30053277e+03,  2.54083478e+00,  8.10541089e-01],\n",
       "        [ 5.75904058e+01,  1.72009625e+00, -1.88743420e-01, ...,\n",
       "          6.99172208e+03,  2.90377637e+00,  3.51554487e-01]]),\n",
       " 'category': ['50-75%', '75-100%', '0-25%', '50-75%', '0-25%', ..., '75-100%', '50-75%', '50-75%', '25-50%', '50-75%']\n",
       " Length: 320\n",
       " Categories (4, object): ['0-25%' < '25-50%' < '50-75%' < '75-100%']}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2e9dfbb-340d-4100-8118-577b05da20d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Per-column arrays must each be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_new \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:637\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    631\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    632\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    633\u001b[0m     )\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 637\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/internals/construction.py:502\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    494\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    495\u001b[0m         x\n\u001b[1;32m    496\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m    497\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m x\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[1;32m    499\u001b[0m     ]\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;66;03m# TODO: can we get rid of the dt64tz special case above?\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/internals/construction.py:120\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/internals/construction.py:661\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    659\u001b[0m         raw_lengths\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(val))\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m val\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 661\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_lengths:\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf using all scalar values, you must pass an index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Per-column arrays must each be 1-dimensional"
     ]
    }
   ],
   "source": [
    "data_new = pd.DataFrame(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9538763c-c9a7-46ea-8d27-687380b01f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.78149680e+01, 5.55483572e+01, 1.17874644e+03, 2.30720834e+03,\n",
       "       1.79747725e+04])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "84f68d05-2c05-4ce4-bd41-e7d392acb970",
   "metadata": {},
   "outputs": [],
   "source": [
    "center = np.mean(X_new, axis=0)\n",
    "dist = [distance.euclidean(point, center) for point in X_new]\n",
    "quant_list = np.quantile(dist, [0.25, 0.5, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "502f8959-e568-49a9-8ad0-998c2970a1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.11635598696819"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a601c139-4abd-43f3-86e5-2c2811853c29",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "p < 0, p > 1 or p contains NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [120]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# dist_scaled = dist_std * (np.max(dist)-np.min(dist)) + np.min(dist)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m phi_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mdist_std\n\u001b[0;32m----> 6\u001b[0m z_list \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphi_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32mmtrand.pyx:3386\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.binomial\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_common.pyx:359\u001b[0m, in \u001b[0;36mnumpy.random._common.check_array_constraint\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: p < 0, p > 1 or p contains NaNs"
     ]
    }
   ],
   "source": [
    "center = np.mean(X_new, axis=0)\n",
    "dist = [distance.euclidean(point, center) for point in X_new]\n",
    "dist_std = (dist-np.min(dist))/(np.max(dist)-np.min(dist))\n",
    "dist_std = [d if d>0 else dfor d in dist_std]\n",
    "# dist_scaled = dist_std * (np.max(dist)-np.min(dist)) + np.min(dist)\n",
    "phi_list = 1/dist_std\n",
    "z_list = np.random.binomial(1, phi_list)\n",
    "# # Random Dropout for X\n",
    "# X_new = X_new_ori*z_list/(1-phi_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "62644283-1db1-411c-90fa-18951753e485",
   "metadata": {},
   "outputs": [],
   "source": [
    "center = np.mean(X_new, axis=0)\n",
    "dist = [distance.euclidean(point, center) for point in X_new]\n",
    "# dist_scaled = dist_std * (np.max(dist)-np.min(dist)) + np.min(dist)\n",
    "phi_list = dist/np.sum(dist)\n",
    "z_list = np.random.binomial(1, phi_list)\n",
    "# # Random Dropout for X\n",
    "# X_new = X_new_ori*z_list/(1-phi_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b4d4a3fd-11aa-4d59-b3e3-09817a0777a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0cf608-ced7-4d16-a4af-c8f835e0f245",
   "metadata": {},
   "source": [
    "按照distance四等分（0-25，25-50...）,tune upper buond for probability (starting from 0.5 to 0.9), assign probability to 4 groups "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
