{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46fd47ad-0a97-4d5a-a6c7-3a358a23b42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f21cd17a-60f4-49da-b36f-8da47e17b769",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "587fe984-54cc-46b1-aa83-30837dd96938",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size = 0.2, random_state = 42)\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30814598-8ba6-4f19-9878-347bad4cacec",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['Class']\n",
    "X_train = train.loc[:, train.columns!='Class']\n",
    "y_test = test['Class']\n",
    "X_test = test.loc[:, test.columns!='Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60a9928a-7982-45d8-a2ef-340810c32754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(algo, X, Y, M, K, criteria):\n",
    "    \"\"\"\n",
    "    This function uses dropout method to train a predicted model that optimizes the specific criterion.\n",
    "    @param algo(required): A learning algorithm that takes X, Y as inputs and outputs a model \n",
    "    @param X(required): The matrix of indepdent/predictor variables\n",
    "    @param Y(required): The outcome variable\n",
    "    @param M(required): The number of Monte Carlo replicates\n",
    "    @param K(required): Number of CV folds for tuning hyperparameter\n",
    "    @param criteria(required): A criterion to be used to evaluate the method. \n",
    "    Can be either mean squared error (MSE) or mean absolute deviation (MAD)\n",
    "    @return: A predictive model that optimizes the specific criterion\n",
    "    \"\"\"\n",
    "    \n",
    "    # List of phi values to be tuned\n",
    "    phi_list = np.arange(0, 1, 0.1)\n",
    "    cv_error = []\n",
    "    \n",
    "    for phi in phi_list:\n",
    "        # Create k folds for cross-validation\n",
    "        kf = KFold(n_splits = K, shuffle=True)\n",
    "        error = []\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "            \n",
    "            # Monte Carlo: expand original data M times to generate a new dataset \n",
    "            # (X will be modified later)\n",
    "            X_new_ori = np.repeat(X_train, M, axis=0)\n",
    "            Y_new = np.repeat(Y_train, M, axis=0)\n",
    "            \n",
    "            # Generate independent Bernoulli random variables Z\n",
    "            z = np.random.binomial(1, phi, size=X_new_ori.shape)\n",
    "            # Random Dropout for X\n",
    "            X_new = X_new_ori*z/(1-phi)\n",
    "            \n",
    "            # Fit input learning algorithm with new data\n",
    "            reg = algo(X_new, Y_new)\n",
    "            # Predict Y using X test\n",
    "            pred = reg.predict(X_test)\n",
    "            \n",
    "            # Calculate MSE/MAD using the model obtained above\n",
    "            if criteria == \"MSE\":\n",
    "                error.append(mean_squared_error(Y_test, pred))\n",
    "            elif criteria == \"MAD\":\n",
    "                error.append(mean_absolute_error(Y_test, pred))\n",
    "            # Raise error if input is not MSE or MAD\n",
    "            else:\n",
    "                raise ValueError('Please input either MSE or MAD!')\n",
    "        \n",
    "        cv_error.append(np.mean(error))\n",
    "    \n",
    "    # Find optimal phi value that has the smallest CV error\n",
    "    phi_opt = phi_list[np.argmin(cv_error)]\n",
    "    \n",
    "    # Use the optimal phi value to train the predicted model\n",
    "    X_new_ori = np.repeat(X, M, axis=0)\n",
    "    Y_new = np.repeat(Y, M, axis=0)\n",
    "    \n",
    "    z = np.random.binomial(1, phi_opt, size=X_new_ori.shape)\n",
    "    X_new = X_new_ori*z/(1-phi_opt)\n",
    "\n",
    "    reg = algo(X_new, Y_new)\n",
    "    \n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e482e6da-33e6-4330-ba75-7d3ff5b6d28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_updated(algo, X, Y, M, K, criteria):\n",
    "    \"\"\"\n",
    "    This function uses dropout method to train a predicted model that optimizes the specific criterion.\n",
    "    @param algo(required): A learning algorithm that takes X, Y as inputs and outputs a model \n",
    "    @param X(required): The matrix of indepdent/predictor variables\n",
    "    @param Y(required): The outcome variable\n",
    "    @param M(required): The number of Monte Carlo replicates\n",
    "    @param K(required): Number of CV folds for tuning hyperparameter\n",
    "    @param criteria(required): A criterion to be used to evaluate the method. \n",
    "    Can be either mean squared error (MSE) or mean absolute deviation (MAD)\n",
    "    @return: A predictive model that optimizes the specific criterion\n",
    "    \"\"\"\n",
    "    \n",
    "    cv_error = []\n",
    "    # Create k folds for cross-validation\n",
    "    kf = KFold(n_splits = K, shuffle=True)\n",
    "    error = []\n",
    "    # a list of upper bounds for probability \n",
    "    p_up_list = np.arange(0.5, 1, 0.1)\n",
    "    \n",
    "    # tune the upper bound for probability\n",
    "    for p_up in p_up_list:\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "            # Monte Carlo: expand original data M times to generate a new dataset \n",
    "            # (X will be modified later)\n",
    "            X_new_ori = np.repeat(X_train, M, axis=0)\n",
    "            Y_new = np.repeat(Y_train, M, axis=0)\n",
    "\n",
    "            # Calculate the centroid for the data \n",
    "            center = np.mean(X_new_ori, axis=0)\n",
    "            # Euclidean distance (feature importance) for each observation\n",
    "            dist = [distance.euclidean(point, center) for point in X_new_ori]\n",
    "            # Seperate distances into 4 categories (ex: 25% quantile...)\n",
    "            quant_list = np.quantile(dist, [0, 0.25, 0.5, 0.75, 1])\n",
    "            dist_df = pd.DataFrame({'dist': dist})\n",
    "            # Create another column for the category \n",
    "            dist_df['category'] = pd.cut(dist_df['dist'], quant_list, \n",
    "                                         labels=['0-25%', '25-50%', '50-75%', '75-100%'])\n",
    "            dist_df['prob'] = 0\n",
    "            # Assign probabilities for each category\n",
    "            # The larger distance, the lower probability to be dropped \n",
    "            dist_df.loc[dist_df.category == '0-25%', 'prob'] = p_up\n",
    "            dist_df.loc[dist_df.category == '25-50%', 'prob'] = p_up - 0.1\n",
    "            dist_df.loc[dist_df.category == '50-75%', 'prob'] = p_up - 0.2\n",
    "            dist_df.loc[dist_df.category == '75-100%', 'prob'] = p_up - 0.3\n",
    "            phi_list = dist_df['prob'].values\n",
    "            phi_list = phi_list.reshape(-1,1)\n",
    "            # Generate independent Bernoulli random variables Z in terms of \n",
    "            # different probabilities for each observation \n",
    "            z_list = np.random.binomial(1, phi_list, size=X_new_ori.shape)\n",
    "\n",
    "            # Random Dropout for X\n",
    "            X_new = X_new_ori*z_list/(1-phi_list)\n",
    "\n",
    "            # Fit input learning algorithm with new data\n",
    "            reg = algo(X_new, Y_new)\n",
    "            # Predict Y using X test\n",
    "            pred = reg.predict(X_test)\n",
    "\n",
    "            # Calculate MSE/MAD using the model obtained above\n",
    "            if criteria == \"MSE\":\n",
    "                error.append(mean_squared_error(Y_test, pred))\n",
    "            elif criteria == \"MAD\":\n",
    "                error.append(mean_absolute_error(Y_test, pred))\n",
    "            # Raise error if input is not MSE or MAD\n",
    "            else:\n",
    "                raise ValueError('Please input either MSE or MAD!')\n",
    "\n",
    "        cv_error.append(np.mean(error))\n",
    "    \n",
    "    # Find optimal upper bound for probability value that has the smallest CV error\n",
    "    p_up_opt = p_up_list[np.argmin(cv_error)]\n",
    "    \n",
    "    # Use the optimal p_up value to train the predicted model\n",
    "    X_new_ori = np.repeat(X, M, axis=0)\n",
    "    Y_new = np.repeat(Y, M, axis=0)\n",
    "    \n",
    "    # Generate independent Bernoulli random variables Z\n",
    "    center = np.mean(X_new_ori, axis=0)\n",
    "    dist = [distance.euclidean(point, center) for point in X_new_ori]\n",
    "    quant_list = np.quantile(dist, [0, 0.25, 0.5, 0.75, 1])\n",
    "    dist_df = pd.DataFrame({'dist': dist})\n",
    "    dist_df['category'] = pd.cut(dist_df['dist'], quant_list, \n",
    "                                 labels=['0-25%', '25-50%', '50-75%', '75-100%'])\n",
    "    dist_df['prob'] = 0\n",
    "    dist_df.loc[dist_df.category == '0-25%', 'prob'] = p_up_opt\n",
    "    dist_df.loc[dist_df.category == '25-50%', 'prob'] = p_up_opt - 0.1\n",
    "    dist_df.loc[dist_df.category == '50-75%', 'prob'] = p_up_opt - 0.2\n",
    "    dist_df.loc[dist_df.category == '75-100%', 'prob'] = p_up_opt - 0.3\n",
    "    phi_list = dist_df['prob'].values\n",
    "    phi_list = phi_list.reshape(-1,1)\n",
    "    z_list = np.random.binomial(1, phi_list, size=X_new_ori.shape)\n",
    "\n",
    "    # Random Dropout for X\n",
    "    X_new = X_new_ori*z_list/(1-phi_list)\n",
    "\n",
    "    # Fit input learning algorithm with new data\n",
    "    reg = algo(X_new, Y_new)\n",
    "    \n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "30f0cde2-d657-4bb0-8964-b9c9c168df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgt_drop(X, Y):\n",
    "    return LogisticRegression(random_state=42, penalty='none').fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "bc83b964-797c-4ce2-a92f-54f4f760ad16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9125"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgt = LogisticRegression(random_state=42, penalty='none')\n",
    "lgt.fit(X_train, y_train)\n",
    "np.mean(lgt.predict(X_test) == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5c00ad58-70af-4732-afe5-120cf693a473",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train.values\n",
    "y = y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f798b52d-a1e8-4b1f-996a-c81829422df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgt_drop_fit = dropout(lgt_drop, X, y, 100, 5, 'MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e64251d0-fdc5-4a4b-8479-8ddec1afa48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.925"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lgt_drop_fit.predict(X_test) == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "de25abb7-0f0f-4f48-986f-6447c3223bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgt_dropup_fit = dropout_updated(lgt_drop, X, y, 100, 5, 'MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "9328ce8b-561b-491c-9cb7-7413bb1d65ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9375"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lgt_dropup_fit.predict(X_test) == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8b5f9c-2fd0-4816-95ba-ff7facd4491d",
   "metadata": {},
   "source": [
    "The above indicates that the updated dropout has the best performance among original dropout and logistic regression model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
