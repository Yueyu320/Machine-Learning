{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d217e070-6237-41cd-9394-a689ff5401a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f3fc5692-acd5-490e-ad3f-d5c8f10f68e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blackbox(algo, X, Y, method = \"Dropout\", M = 100, K = 5, criteria = \"MSE\"):\n",
    "    if method == \"Dropout\":\n",
    "        return dropout(algo, X, Y, M, K, criteria)\n",
    "    elif method == \"NoiseAddition\":\n",
    "        return noiseAddition_M(algo, X, Y, M, K, criteria)\n",
    "    elif method == \"Robust\":\n",
    "        return Robust(algo, X, Y, K, criteria)\n",
    "    else:\n",
    "        raise ValueError('Method should be one of Dropout, NoiseAddition or Robust!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9648234d-ce23-4bf8-a76b-540d470f4f19",
   "metadata": {},
   "source": [
    "This function searchs a list of $\\phi$ values in the range of (0, 1) and finds the optimal $\\phi$ that yields to the smallest value of a criterion of user's choice (either MSE or MAD) using the dropout method. \n",
    "\n",
    "For each candidate $\\phi$, a K-fold cross validation is conducted. In each fold, the data is split into train and test set. The train set is first repeatedly expanded M times with itself for easier computation purpose. Then, an independent Bernoulli random variables Z is generated with probability $\\phi$. The random dropout is created by multiplying each element of X and Z, and then dividing the product by $1-\\phi$. It then becomes the new X train data and is fed into the learning algorithm together with the expanded Y train data. A prediction is made on the test X with the model returned by the learning algorithm and is used to calculate the criterion (MSE or MAD based on user's choice). After iterating through all folds, the average CV error is obtained and will be compared with that using different values of $\\phi$.\n",
    "\n",
    "Once all $\\phi$ values are examined, the optimal $\\phi$ with smallest CV errors is selected to train the final output model. The entire X and Y are expanded M times with themselves, independent Bernoulli variable Z is generated as before but with the optimal $\\phi$ as its probability. Once X is modified, the learning algorithm is used to train the new X and Y data, and the returning function will be the final output of this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66ed78d1-bda2-48c3-8207-f2e67e948179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(algo, X, Y, M, K, criteria):\n",
    "    \"\"\"\n",
    "    This function uses dropout method to train a predicted model that optimizes the specific criterion.\n",
    "    @param algo: A learning algorithm that takes X, Y as inputs and outputs a model \n",
    "    @param X: The matrix of indepdent variables\n",
    "    @param Y: The outcome variable\n",
    "    @param M: The number of Monte Carlo replicates\n",
    "    @param K: Number of CV folds for tuning hyperparameterr\n",
    "    @param criteria: A criterion to be used to evaluate the method. \n",
    "    Can be either mean squared error (MSE) or mean absolute deviation (MAD)\n",
    "    @return: A predictive model that optimizes the specific criterion\n",
    "    \"\"\"\n",
    "    \n",
    "    # List of phi values to be tuned\n",
    "    phi_list = np.arange(0, 1, 0.1)\n",
    "    cv_error = []\n",
    "    \n",
    "    for phi in phi_list:\n",
    "        # Create k folds for cross-validation\n",
    "        kf = KFold(n_splits = K, shuffle=True)\n",
    "        error = []\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "            \n",
    "            # Monte Carlo: expand original data M times to generate a new dataset \n",
    "            # (X will be modified later)\n",
    "            X_new_ori = np.repeat(X_train, M, axis=0)\n",
    "            Y_new = np.repeat(Y_train, M, axis=0)\n",
    "            \n",
    "            # Generate independent Bernoulli random variables Z\n",
    "            z = np.random.binomial(1, phi, size=X_new_ori.shape)\n",
    "            # Random Dropout for X\n",
    "            X_new = X_new_ori*z/(1-phi)\n",
    "            #X_new = (X_new - np.mean(X_new, axis=0))/np.std(X_new, axis=0)\n",
    "            \n",
    "            # Fit input learning algorithm with new data\n",
    "            reg = algo(X_new, Y_new)\n",
    "            # Predict Y using X test\n",
    "            pred = reg.predict(X_test)\n",
    "            \n",
    "            # Calculate MSE/MAD using the model obtained above\n",
    "            if criteria == \"MSE\":\n",
    "                error.append(mean_squared_error(Y_test, pred))\n",
    "            elif criteria == \"MAD\":\n",
    "                error.append(mean_absolute_error(Y_test, pred))\n",
    "            # Raise error if input is not MSE or MAD\n",
    "            else:\n",
    "                raise ValueError('Please input either MSE or MAD!')\n",
    "        \n",
    "        cv_error.append(np.mean(error))\n",
    "    \n",
    "    # Find optimal phi value that has the smallest CV error\n",
    "    phi_opt = phi_list[np.argmin(cv_error)]\n",
    "    \n",
    "    # Use the optimal phi value to train the predicted model\n",
    "    X_new_ori = np.repeat(X, M, axis=0)\n",
    "    Y_new = np.repeat(Y, M, axis=0)\n",
    "\n",
    "    z = np.random.binomial(1, phi_opt, size=X_new_ori.shape)\n",
    "    X_new = X_new_ori*z/(1-phi_opt)\n",
    "    #X_new = (X_new - np.mean(X_new, axis=0))/np.std(X_new, axis=0)\n",
    "\n",
    "    reg = algo(X_new, Y_new)\n",
    "    \n",
    "    return reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920b2a3c-5b1c-4b4c-80b8-5101b4f5076e",
   "metadata": {},
   "source": [
    "This function searchs a list of $\\lambda$ values in the range of (0, 1) and finds the optimal $\\phi$ that yields to the smallest value of a criterion of user's choice (either MSE or MAD) using the noise addition method. \n",
    "\n",
    "For each candidate $\\lambda$, a K-fold cross validation is conducted. In each fold, the data is split into train and test set. The train set is first repeatedly expanded M times with itself for easier computation purpose. Then, a random noise Z which is an independent Normal random variables with mean 0 and variance $\\lambda$ is added to X. It then becomes the new X train data and is fed into the learning algorithm together with the expanded Y train data. A prediction is made on the test X with the model returned by the learning algorithm and is used to calculate the criterion (MSE or MAD based on user's choice). After iterating through all folds, the average CV error is obtained and will be compared with that using different values of $\\lambda$.\n",
    "\n",
    "Once all $\\lambda$ values are examined, the optimal $\\lambda$ with smallest CV errors is selected to train the final output model. The entire X and Y are expanded M times with themselves, independent normal variable Z is added to X as before but with the variance being optimal $\\lambda$. Once X is modified, the learning algorithm is used to train the new X and Y data, and the returning function will be the final output of this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ec057a9e-f80e-4748-a9bd-149604533aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noiseAddition_M(algo, X, Y, M, K, criteria):\n",
    "    \"\"\"\n",
    "    This function uses noise addition method to train a predicted model that optimizes the specific criterion.\n",
    "    @param algo: A learning algorithm that takes X, Y as inputs and outputs a model \n",
    "    @param X: The matrix of indepdent variables\n",
    "    @param Y: The outcome variable\n",
    "    @param M: The number of Monte Carlo replicates\n",
    "    @param K: Number of CV folds for tuning hyperparameterr\n",
    "    @param criteria: A criterion to be used to evaluate the method. \n",
    "    Can be either mean squared error (MSE) or mean absolute deviation (MAD)\n",
    "    @return: A predictive model that optimizes the specific criterion\n",
    "    \"\"\"\n",
    "    \n",
    "    lambda_list = np.arange(0, 1, 0.1)\n",
    "    cv_error = []\n",
    "    \n",
    "    for l in lambda_list:\n",
    "        kf = KFold(n_splits = K, shuffle=True)\n",
    "        error = []\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "            \n",
    "            X_new_ori = np.repeat(X_train, M, axis=0)\n",
    "            Y_new = np.repeat(Y_train, M, axis=0)\n",
    "\n",
    "            z = np.random.normal(0, l, size=X_new_ori.shape)\n",
    "            X_new = X_new_ori + z\n",
    "            #X_new = (X_new - np.mean(X_new, axis=0))/np.std(X_new, axis=0)\n",
    "            \n",
    "            reg = algo(X_new, Y_new)\n",
    "            pred = reg.predict(X_test)\n",
    "\n",
    "            if criteria == \"MSE\":\n",
    "                error.append(mean_squared_error(Y_test, pred))\n",
    "            elif criteria == \"MAD\":\n",
    "                error.append(mean_absolute_error(Y_test, pred))\n",
    "            else:\n",
    "                raise ValueError('Please input either MSE or MAD!')\n",
    "        \n",
    "        cv_error.append(np.mean(error))\n",
    "        \n",
    "    l_opt = lambda_list[np.argmin(cv_error)]\n",
    "    \n",
    "    X_new_ori = np.repeat(X, M, axis=0)\n",
    "    Y_new = np.repeat(Y, M, axis=0)\n",
    "\n",
    "    z = np.random.normal(0, l_opt, size=X_new_ori.shape)\n",
    "    X_new = X_new_ori + z\n",
    "    #X_new = (X_new - np.mean(X_new, axis=0))/np.std(X_new, axis=0)\n",
    "\n",
    "    reg = algo(X_new, Y_new)\n",
    "    \n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52dd7a46-cb76-440a-9010-042143923935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noiseAddition(algo, X, Y, M, K, criteria):\n",
    "    lambda_list = np.arange(0, 1, 0.1)\n",
    "    cv_error = []\n",
    "    \n",
    "    for l in lambda_list:\n",
    "        kf = KFold(n_splits = K, shuffle=True)\n",
    "        error = []\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "            z = np.random.normal(0, l, size=X_train.shape)\n",
    "            X_new = X_train + z\n",
    "            #X_new = (X_new - np.mean(X_new, axis=0))/np.std(X_new, axis=0)\n",
    "\n",
    "            reg = algo(X_new, Y_train)\n",
    "            pred = reg.predict(X_test)\n",
    "\n",
    "            if criteria == \"MSE\":\n",
    "                error.append(mean_squared_error(Y_test, pred))\n",
    "            elif criteria == \"MAD\":\n",
    "                error.append(mean_absolute_error(Y_test, pred))\n",
    "            else:\n",
    "                raise ValueError('Please input either MSE or MAD!')\n",
    "        \n",
    "        cv_error.append(np.mean(error))\n",
    "        \n",
    "    l_opt = lambda_list[np.argmin(cv_error)]\n",
    "    \n",
    "    z = np.random.normal(0, l_opt, size=X.shape)\n",
    "    #X_new = (X_new - np.mean(X_new, axis=0))/np.std(X_new, axis=0)\n",
    "\n",
    "    reg = algo(X+z, Y)\n",
    "    \n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ec5572af-bf01-4942-9db8-726a004c14ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Robust(algo, X, Y, K, criteria):\n",
    "    \"\"\"\n",
    "    This function uses robust method to train a predicted model that optimizes the specific criterion.\n",
    "    @param algo: A learning algorithm that takes X, Y as inputs and outputs a model \n",
    "    @param X: The matrix of indepdent variables\n",
    "    @param Y: The outcome variable\n",
    "    @param K: Number of CV folds for tuning hyperparameterr\n",
    "    @param criteria: A criterion to be used to evaluate the method. \n",
    "    Can be either mean squared error (MSE) or mean absolute deviation (MAD)\n",
    "    @return: A predictive model that optimizes the specific criterion\n",
    "    \"\"\"\n",
    "    num_pre = X.shape[1]\n",
    "    cj_sum_list = np.arange(0, 1, 0.1)\n",
    "    c_opt = [0]*num_pre\n",
    "    cv_error = 100000\n",
    "    \n",
    "    for c_sum in cj_sum_list:\n",
    "        c = np.random.randint(1, 10, size=num_pre)\n",
    "        c = c/sum(c) * c_sum\n",
    "        \n",
    "        kf = KFold(n_splits = K, shuffle=True)\n",
    "        error = []\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "            \n",
    "            first = 0\n",
    "            num_row = X_train.shape[0]\n",
    "            for j in range(num_pre):\n",
    "                first += 1\n",
    "                # randomly sample a value k such that k <= c_j^2\n",
    "                k = np.random.uniform(low=0.001, high=c[j]**2, size=1)[0]\n",
    "                delta = np.random.randint(1, 10, size=num_row)\n",
    "                delta = delta/sum(delta) * k\n",
    "                delta = np.sqrt(delta)\n",
    "                \n",
    "                if first == 1:\n",
    "                    delta_matrix = delta\n",
    "                else:\n",
    "                    delta_matrix = np.column_stack((delta_matrix, delta))\n",
    "                    \n",
    "            X_new = X_train + delta_matrix\n",
    "            #X_new = (X_new - np.mean(X_new, axis=0))/np.std(X_new, axis=0)\n",
    "\n",
    "            reg = algo(X_new, Y_train)\n",
    "            pred = reg.predict(X_test)\n",
    "\n",
    "            if criteria == \"MSE\":\n",
    "                error.append(mean_squared_error(Y_test, pred))\n",
    "            elif criteria == \"MAD\":\n",
    "                error.append(mean_absolute_error(Y_test, pred))\n",
    "            else:\n",
    "                raise ValueError('Please input either MSE or MAD!')\n",
    "        \n",
    "        if np.mean(error) < cv_error:\n",
    "            cv_error = np.mean(error)\n",
    "            c_opt = c\n",
    "    \n",
    "    first = 0\n",
    "    num_row = X.shape[0]\n",
    "    for j in range(num_pre):\n",
    "        first += 1\n",
    "        # randomly sample a value k such that k <= c_j^2\n",
    "        k = np.random.uniform(low=0.001, high=c_opt[j]**2, size=1)[0]\n",
    "        delta = np.random.randint(1, 10, size=num_row)\n",
    "        delta = delta/sum(delta) * k\n",
    "        delta = np.sqrt(delta)\n",
    "\n",
    "        if first == 1:\n",
    "            delta_matrix = delta\n",
    "        else:\n",
    "            delta_matrix = np.column_stack((delta_matrix, delta))\n",
    "            \n",
    "    X_new = X + delta_matrix\n",
    "    #X_new = (X_new - np.mean(X_new, axis=0))/np.std(X_new, axis=0)\n",
    "\n",
    "    reg = algo(X_new, Y)\n",
    "    \n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70b32e66-32d5-48e0-a130-23d75ad828b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_table('dataset.dat', sep='\\s+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0178a1e0-6c6c-4087-8583-072f8954ce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def algorithm(X, Y):\n",
    "    return LinearRegression().fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "87e5d56a-d120-486b-a936-5387de5867e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.values[:,:8]\n",
    "Y = data['lpsa'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b304c7ea-0408-44f8-b0b1-d1f331b3ff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod1_mse = blackbox(algorithm, X, Y, method = \"Dropout\", K = 5, criteria = \"MSE\")\n",
    "mod1_mad = blackbox(algorithm, X, Y, method = \"Dropout\", K = 5, criteria = \"MAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5c70d8ef-d825-4620-a429-40b2397684df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.676075654684088\n",
      "0.6628653328647743\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(Y, mod1_mse.predict(X)))\n",
    "print(mean_absolute_error(Y, mod1_mad.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7e9d0146-f842-44fe-878d-c1d60adf52ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod2_mse = blackbox(algorithm, X, Y, method = \"NoiseAddition\", K = 5, criteria = \"MSE\")\n",
    "mod2_mad = blackbox(algorithm, X, Y, method = \"NoiseAddition\", K = 5, criteria = \"MAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9c240258-053d-46ad-aed5-bd9e8e696549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4445726594315684\n",
      "0.5083114292194318\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(Y, mod2_mse.predict(X)))\n",
    "print(mean_absolute_error(Y, mod2_mad.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6f30115a-a5cd-4b81-8487-98b1df3062d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod3_mse = blackbox(algorithm, X, Y, method = \"Robust\", K = 5, criteria = \"MSE\")\n",
    "mod3_mad = blackbox(algorithm, X, Y, method = \"Robust\", K = 5, criteria = \"MAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "30e536c1-918d-4581-bd06-08dc708ae6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4440847303856871\n",
      "0.5055265041483498\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(Y, mod3_mse.predict(X)))\n",
    "print(mean_absolute_error(Y, mod3_mad.predict(X)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
