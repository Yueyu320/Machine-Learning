{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01955d2e-79cd-4d99-be2a-dba8bd1006c3",
   "metadata": {},
   "source": [
    "# STA 561 - HW3\n",
    "### Nancy Huang, Camilla Yu, Yihan Ma, Eva Gao, Yaze Gao, Shuo Wang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d217e070-6237-41cd-9394-a689ff5401a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cb331d-6905-4c38-9f21-c241904df618",
   "metadata": {},
   "source": [
    "## Function for Tuning Blackbox Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3fc5692-acd5-490e-ad3f-d5c8f10f68e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blackbox(algo, X, Y, method = \"Dropout\", M = 100, K = 5, c = None, criteria = \"MSE\"):\n",
    "    \"\"\"\n",
    "    This function uses different regularization methods to train a predicted model that optimizes the specific criterion.\n",
    "    @param algo(required): A learning algorithm that takes X, Y as inputs and outputs a model \n",
    "    @param X(required): The matrix of indepdent/predictor variables\n",
    "    @param Y(required): The outcome variable\n",
    "    @param method(default value is 'Dropout'): Regularization methods \n",
    "    Can be 'Dropout', 'NoiseAddition' or 'Robust'\n",
    "    @param M(default value is 100): The number of Monte Carlo replicates\n",
    "    @param K(default value is 5): Number of CV folds for tuning hyperparameter\n",
    "    @param c(only used when the method is 'Robust'): A kxp matrix (k: the number of hyperparameters users want to try, \n",
    "    p: the number of predictor variables in X). The column of c means the column bounds for X or the bound for each \n",
    "    predictor variable. The row of c means how many hyperparameters users want to tune to find the best one.\n",
    "    The best hyperparameter here is a p-dimensional vector.\n",
    "    @param criteria(default value is 'MSE'): A criterion to be used to evaluate the method\n",
    "    Can be either mean squared error (MSE) or mean absolute deviation (MAD)\n",
    "    @return: A predictive model that optimizes the specific criterion\n",
    "    \"\"\"\n",
    "    \n",
    "    if method == \"Dropout\":\n",
    "        return dropout(algo, X, Y, M, K, criteria)\n",
    "    elif method == \"NoiseAddition\":\n",
    "        return noiseAddition(algo, X, Y, M, K, criteria)\n",
    "    elif method == \"Robust\":\n",
    "        return Robust(algo, X, Y, K, c, criteria)\n",
    "    else:\n",
    "        raise ValueError('Method should be one of Dropout, NoiseAddition or Robust!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9648234d-ce23-4bf8-a76b-540d470f4f19",
   "metadata": {},
   "source": [
    "`dropout` function searchs a list of $\\phi$ values in the range of (0, 1) and finds the optimal $\\phi$ that yields to the smallest value of a criterion of user's choice (either MSE or MAD) using the dropout method. \n",
    "\n",
    "For each candidate $\\phi$, a K-fold cross validation is conducted. In each fold, the data is split into train and test set. The train set is first repeatedly expanded M times with itself for easier computation purpose. Then, an independent Bernoulli random variables Z is generated with probability $\\phi$. The random dropout is created by multiplying each element of X and Z, and then dividing the product by $1-\\phi$. It then becomes the new X train data and is fed into the learning algorithm together with the expanded Y train data. A prediction is made on the test X with the model returned by the learning algorithm and is used to calculate the criterion (MSE or MAD based on user's choice). After iterating through all folds, the average CV error is obtained and will be compared with that using different values of $\\phi$.\n",
    "\n",
    "Once all $\\phi$ values are examined, the optimal $\\phi$ with smallest CV errors is selected to train the final output model. The entire X and Y are expanded M times with themselves, independent Bernoulli variable Z is generated as before but with the optimal $\\phi$ as its probability. Once X is modified, the learning algorithm is used to train the new X and Y data, and the returning function will be the final output of this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66ed78d1-bda2-48c3-8207-f2e67e948179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(algo, X, Y, M, K, criteria):\n",
    "    \"\"\"\n",
    "    This function uses dropout method to train a predicted model that optimizes the specific criterion.\n",
    "    @param algo(required): A learning algorithm that takes X, Y as inputs and outputs a model \n",
    "    @param X(required): The matrix of indepdent/predictor variables\n",
    "    @param Y(required): The outcome variable\n",
    "    @param M(required): The number of Monte Carlo replicates\n",
    "    @param K(required): Number of CV folds for tuning hyperparameter\n",
    "    @param criteria(required): A criterion to be used to evaluate the method. \n",
    "    Can be either mean squared error (MSE) or mean absolute deviation (MAD)\n",
    "    @return: A predictive model that optimizes the specific criterion\n",
    "    \"\"\"\n",
    "    \n",
    "    # List of phi values to be tuned\n",
    "    phi_list = np.arange(0, 1, 0.1)\n",
    "    cv_error = []\n",
    "    \n",
    "    for phi in phi_list:\n",
    "        # Create k folds for cross-validation\n",
    "        kf = KFold(n_splits = K, shuffle=True)\n",
    "        error = []\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "            \n",
    "            # Monte Carlo: expand original data M times to generate a new dataset \n",
    "            # (X will be modified later)\n",
    "            X_new_ori = np.repeat(X_train, M, axis=0)\n",
    "            Y_new = np.repeat(Y_train, M, axis=0)\n",
    "            \n",
    "            # Generate independent Bernoulli random variables Z\n",
    "            z = np.random.binomial(1, phi, size=X_new_ori.shape)\n",
    "            # Random Dropout for X\n",
    "            X_new = X_new_ori*z/(1-phi)\n",
    "            \n",
    "            # Fit input learning algorithm with new data\n",
    "            reg = algo(X_new, Y_new)\n",
    "            # Predict Y using X test\n",
    "            pred = reg.predict(X_test)\n",
    "            \n",
    "            # Calculate MSE/MAD using the model obtained above\n",
    "            if criteria == \"MSE\":\n",
    "                error.append(mean_squared_error(Y_test, pred))\n",
    "            elif criteria == \"MAD\":\n",
    "                error.append(mean_absolute_error(Y_test, pred))\n",
    "            # Raise error if input is not MSE or MAD\n",
    "            else:\n",
    "                raise ValueError('Please input either MSE or MAD!')\n",
    "        \n",
    "        cv_error.append(np.mean(error))\n",
    "    \n",
    "    # Find optimal phi value that has the smallest CV error\n",
    "    phi_opt = phi_list[np.argmin(cv_error)]\n",
    "    \n",
    "    # Use the optimal phi value to train the predicted model\n",
    "    X_new_ori = np.repeat(X, M, axis=0)\n",
    "    Y_new = np.repeat(Y, M, axis=0)\n",
    "    \n",
    "    z = np.random.binomial(1, phi_opt, size=X_new_ori.shape)\n",
    "    X_new = X_new_ori*z/(1-phi_opt)\n",
    "\n",
    "    reg = algo(X_new, Y_new)\n",
    "    \n",
    "    return reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920b2a3c-5b1c-4b4c-80b8-5101b4f5076e",
   "metadata": {},
   "source": [
    "`noiseAddition` function searchs a list of $\\lambda$ values in the range of (0, 1) and finds the optimal $\\lambda$ that yields to the smallest value of a criterion of user's choice (either MSE or MAD) using the noise addition method. \n",
    "\n",
    "For each candidate $\\lambda$, a K-fold cross validation is conducted. In each fold, the data is split into train and test set. The train set is first repeatedly expanded M times with itself for easier computation purpose. Then, a random noise Z which is an independent Normal random variables with mean 0 and variance $\\lambda$ is added to X. It then becomes the new X train data and is fed into the learning algorithm together with the expanded Y train data. A prediction is made on the test X with the model returned by the learning algorithm and is used to calculate the criterion (MSE or MAD based on user's choice). After iterating through all folds, the average CV error is obtained and will be compared with that using different values of $\\lambda$.\n",
    "\n",
    "Once all $\\lambda$ values are examined, the optimal $\\lambda$ with smallest CV errors is selected to train the final output model. The entire X and Y are expanded M times with themselves, independent normal variable Z is added to X as before but with the variance being optimal $\\lambda$. Once X is modified, the learning algorithm is used to train the new X and Y data, and the returning function will be the final output of this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec057a9e-f80e-4748-a9bd-149604533aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noiseAddition(algo, X, Y, M, K, criteria):\n",
    "    \"\"\"\n",
    "    This function uses noiseaddition method to train a predicted model that optimizes the specific criterion.\n",
    "    @param algo(required): A learning algorithm that takes X, Y as inputs and outputs a model \n",
    "    @param X(required): The matrix of indepdent/predictor variables\n",
    "    @param Y(required): The outcome variable\n",
    "    @param M(required): The number of Monte Carlo replicates\n",
    "    @param K(required): Number of CV folds for tuning hyperparameter\n",
    "    @param criteria(required): A criterion to be used to evaluate the method. \n",
    "    Can be either mean squared error (MSE) or mean absolute deviation (MAD)\n",
    "    @return: A predictive model that optimizes the specific criterion\n",
    "    \"\"\"\n",
    "    \n",
    "    lambda_list = np.arange(0, 1, 0.1)\n",
    "    cv_error = []\n",
    "    \n",
    "    for l in lambda_list:\n",
    "        kf = KFold(n_splits = K, shuffle=True)\n",
    "        error = []\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "            \n",
    "            X_new_ori = np.repeat(X_train, M, axis=0)\n",
    "            Y_new = np.repeat(Y_train, M, axis=0)\n",
    "            \n",
    "            z = np.random.normal(0, l, size=X_new_ori.shape)\n",
    "            X_new = X_new_ori + z\n",
    "            \n",
    "            reg = algo(X_new, Y_new)\n",
    "            pred = reg.predict(X_test)\n",
    "\n",
    "            if criteria == \"MSE\":\n",
    "                error.append(mean_squared_error(Y_test, pred))\n",
    "            elif criteria == \"MAD\":\n",
    "                error.append(mean_absolute_error(Y_test, pred))\n",
    "            else:\n",
    "                raise ValueError('Please input either MSE or MAD!')\n",
    "        \n",
    "        cv_error.append(np.mean(error))\n",
    "        \n",
    "    l_opt = lambda_list[np.argmin(cv_error)]\n",
    "    \n",
    "    X_new_ori = np.repeat(X, M, axis=0)\n",
    "    Y_new = np.repeat(Y, M, axis=0)\n",
    "    \n",
    "    z = np.random.normal(0, l_opt, size=X_new_ori.shape)\n",
    "    X_new = X_new_ori + z\n",
    "\n",
    "    reg = algo(X_new, Y_new)\n",
    "    \n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec5572af-bf01-4942-9db8-726a004c14ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Robust(algo, X, Y, K, c, criteria):\n",
    "    \"\"\"\n",
    "    This function uses robust method to train a predicted model that optimizes the specific criterion.\n",
    "    @param algo(required): A learning algorithm that takes X, Y as inputs and outputs a model \n",
    "    @param X(required): The matrix of indepdent/predictor variables\n",
    "    @param Y(required): The outcome variable\n",
    "    @param K(required): Number of CV folds for tuning hyperparameter\n",
    "    @param c(required): A kxp matrix (k: the number of hyperparameters users want to try, \n",
    "    p: the number of predictor variables in X). The column of c means the column bounds for X or the bound for each \n",
    "    predictor variable. The row of c means how many hyperparameters users want to tune to find the best one.\n",
    "    The best hyperparameter here is a p-dimensional vector.\n",
    "    @param criteria(required): A criterion to be used to evaluate the method. \n",
    "    Can be either mean squared error (MSE) or mean absolute deviation (MAD)\n",
    "    @return: A predictive model that optimizes the specific criterion\n",
    "    \"\"\"\n",
    "    \n",
    "    num_pre = X.shape[1]\n",
    "    c_opt = [0]*num_pre\n",
    "    cv_error = 100000\n",
    "    \n",
    "    for i in range(c.shape[1]):\n",
    "        c_curr = c[:,i]\n",
    "        kf = KFold(n_splits = K, shuffle=True)\n",
    "        error = []\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "            \n",
    "            first = 0\n",
    "            num_row = X_train.shape[0]\n",
    "            for j in range(num_pre):\n",
    "                first += 1\n",
    "                # randomly sample a value k such that k <= c_j^2\n",
    "                k = np.random.uniform(low=0.001, high=c_curr[j]**2, size=1)[0]\n",
    "                delta = np.random.randint(1, 10, size=num_row)\n",
    "                delta = delta/sum(delta) * k\n",
    "                delta = np.sqrt(delta)\n",
    "                \n",
    "                if first == 1:\n",
    "                    delta_matrix = delta\n",
    "                else:\n",
    "                    delta_matrix = np.column_stack((delta_matrix, delta))\n",
    "                    \n",
    "            X_new = X_train + delta_matrix\n",
    "            \n",
    "            reg = algo(X_new, Y_train)\n",
    "            pred = reg.predict(X_test)\n",
    "\n",
    "            if criteria == \"MSE\":\n",
    "                error.append(mean_squared_error(Y_test, pred))\n",
    "            elif criteria == \"MAD\":\n",
    "                error.append(mean_absolute_error(Y_test, pred))\n",
    "            else:\n",
    "                raise ValueError('Please input either MSE or MAD!')\n",
    "        \n",
    "        if np.mean(error) < cv_error:\n",
    "            cv_error = np.mean(error)\n",
    "            c_opt = c_curr\n",
    "    \n",
    "    first = 0\n",
    "    num_row = X.shape[0]\n",
    "    for j in range(num_pre):\n",
    "        first += 1\n",
    "        # randomly sample a value k such that k <= c_j^2\n",
    "        k = np.random.uniform(low=0.001, high=c_opt[j]**2, size=1)[0]\n",
    "        delta = np.random.randint(1, 10, size=num_row)\n",
    "        delta = delta/sum(delta) * k\n",
    "        delta = np.sqrt(delta)\n",
    "\n",
    "        if first == 1:\n",
    "            delta_matrix = delta\n",
    "        else:\n",
    "            delta_matrix = np.column_stack((delta_matrix, delta))\n",
    "            \n",
    "    X_new = X + delta_matrix\n",
    "    \n",
    "    reg = algo(X_new, Y)\n",
    "    \n",
    "    return reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d694c3e3-d78b-4a9d-b176-4f3e7f9a4aba",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70b32e66-32d5-48e0-a130-23d75ad828b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_table('dataset.dat', sep='\\s+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87e5d56a-d120-486b-a936-5387de5867e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.values[:,:8]\n",
    "Y = data['lpsa'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2456067-685d-4469-a0d6-1e31356d7439",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = np.array([1,2,3,4,5,6,7,8])\n",
    "c2 = np.array([0.5,1,2.5,4,3,5,1,3])\n",
    "c_mx = np.column_stack((c1,c2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b98052b-2018-4a50-82e3-78fd467ab9fa",
   "metadata": {},
   "source": [
    "### 1. Linear Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca9964d1-ba87-402a-bce0-0c050bb40529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_algo(X, Y):\n",
    "    return LinearRegression().fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b304c7ea-0408-44f8-b0b1-d1f331b3ff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod1_mse = blackbox(linear_algo, X, Y, method = \"Dropout\", K = 5, criteria = \"MSE\")\n",
    "mod1_mad = blackbox(linear_algo, X, Y, method = \"Dropout\", K = 5, criteria = \"MAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c70d8ef-d825-4620-a429-40b2397684df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7188379660019427\n",
      "0.6499431890327839\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(Y, mod1_mse.predict(X)))\n",
    "print(mean_absolute_error(Y, mod1_mad.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e9d0146-f842-44fe-878d-c1d60adf52ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod2_mse = blackbox(linear_algo, X, Y, method = \"NoiseAddition\", K = 5, criteria = \"MSE\")\n",
    "mod2_mad = blackbox(linear_algo, X, Y, method = \"NoiseAddition\", K = 5, criteria = \"MAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c240258-053d-46ad-aed5-bd9e8e696549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44459013175048956\n",
      "0.5220624578018508\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(Y, mod2_mse.predict(X)))\n",
    "print(mean_absolute_error(Y, mod2_mad.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f30115a-a5cd-4b81-8487-98b1df3062d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod3_mse = blackbox(linear_algo, X, Y, method = \"Robust\", K = 5, c = c_mx, criteria = \"MSE\")\n",
    "mod3_mad = blackbox(linear_algo, X, Y, method = \"Robust\", K = 5, c = c_mx, criteria = \"MAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30e536c1-918d-4581-bd06-08dc708ae6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.501669144616783\n",
      "0.5214949677915285\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(Y, mod3_mse.predict(X)))\n",
    "print(mean_absolute_error(Y, mod3_mad.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae55eb1-7de6-494d-949e-8a4cbef4b7fd",
   "metadata": {},
   "source": [
    "### Machine Learning Algorithm (Non-linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "983cd0b1-6700-4273-b5e6-96357f83fcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def machine_learning_algo(X, Y):\n",
    "    return GradientBoostingRegressor().fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77cc7c93-bbed-4934-a027-7aa02825a226",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod1_mse_m = blackbox(machine_learning_algo, X, Y, method = \"Dropout\", K = 5, criteria = \"MSE\")\n",
    "mod1_mad_m = blackbox(machine_learning_algo, X, Y, method = \"Dropout\", K = 5, criteria = \"MAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f01a7f0-df79-4251-ab42-0c3f1701b5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.053529836189304\n",
      "0.7854389479651129\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(Y, mod1_mse_m.predict(X)))\n",
    "print(mean_absolute_error(Y, mod1_mad_m.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec3a0d35-1720-455d-a856-53662c2375e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod2_mse_m = blackbox(machine_learning_algo, X, Y, method = \"NoiseAddition\", K = 5, criteria = \"MSE\")\n",
    "mod2_mad_m = blackbox(machine_learning_algo, X, Y, method = \"NoiseAddition\", K = 5, criteria = \"MAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93bd911a-0f26-4e69-8319-87a981bc4e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0257613805074768\n",
      "0.16235555954633077\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(Y, mod2_mse_m.predict(X)))\n",
    "print(mean_absolute_error(Y, mod2_mad_m.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b855f6fc-f25f-4735-93f9-4fb91f0e684f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod3_mse_m = blackbox(machine_learning_algo, X, Y, method = \"Robust\", K = 5, c = c_mx, criteria = \"MSE\")\n",
    "mod3_mad_m = blackbox(machine_learning_algo, X, Y, method = \"Robust\", K = 5, c = c_mx, criteria = \"MAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b47ef2c-de86-479c-ac74-81d79b0675fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22536417367388287\n",
      "0.3635020259918495\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(Y, mod3_mse_m.predict(X)))\n",
    "print(mean_absolute_error(Y, mod3_mad_m.predict(X)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
